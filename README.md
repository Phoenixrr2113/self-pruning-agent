# Self-Pruning Agent

An experimental Next.js application demonstrating **autonomous context window management** for AI agents. Inspired by the blog post ["Teaching AI Agents to Forget to Stop Forgetting"](https://medium.com/@phoenixrr2113/teaching-ai-agents-to-forget-to-stop-forgetting-what-speed-reading-taught-us-about-context-a2044c3f47d0).

## ğŸ§  The Concept

Traditional AI agents are limited by fixed context windows. As conversations grow, older messages get dropped arbitrarily. This project explores a smarter approach: **let the AI decide what to forget**.

The agent:
1. **Sees its context budget** - Token usage is visible in the system prompt
2. **Tags messages with metadata** - Each message has `[msg:XXX][tokens:N][tally:N]`
3. **Suggests what to prune** - When appropriate, outputs `<prune_suggestions>` with confidence scores
4. **Preserves context** - Generates a summary of pruned content as a breadcrumb
5. **Filter happens server-side** - Approved prunes are filtered on subsequent requests

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          Client                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   useChat   â”‚  â”‚DebugPanel   â”‚  â”‚  Context Budget Display â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       API Route (/api/chat)                       â”‚
â”‚                                                                   â”‚
â”‚  1. Inject metadata [msg:XXX][tokens:N][tally:N]                 â”‚
â”‚  2. Filter previously pruned messages                             â”‚
â”‚  3. Inject context summary breadcrumb                             â”‚
â”‚  4. Send to model                                                 â”‚
â”‚  5. Parse <prune_suggestions> from response                       â”‚
â”‚  6. Store context summary + pruned IDs for next request          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
app/src/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ chat/route.ts      # Main chat API with pruning logic
â”‚   â”‚   â””â”€â”€ usage/route.ts     # Token usage endpoint
â”‚   â””â”€â”€ page.tsx               # Chat UI
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ context-budget.tsx     # Token budget progress bar
â”‚   â”œâ”€â”€ debug-panel.tsx        # Event log viewer
â”‚   â”œâ”€â”€ prune-archive.tsx      # Archived pruned messages
â”‚   â””â”€â”€ prune-settings.tsx     # Pruning configuration UI
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ middleware/
â”‚   â”‚   â”œâ”€â”€ metadata-injector.ts   # Adds [msg:XXX] tags to messages
â”‚   â”‚   â”œâ”€â”€ prune-parser.ts        # Extracts <prune_suggestions> XML
â”‚   â”‚   â”œâ”€â”€ prune-executor.ts      # Filters pruned messages
â”‚   â”‚   â””â”€â”€ token-counter.ts       # Tiktoken-based token counting
â”‚   â”œâ”€â”€ prompts/
â”‚   â”‚   â””â”€â”€ system.ts          # System prompt with pruning instructions
â”‚   â”œâ”€â”€ usage-store.ts         # Global state for usage + prune tracking
â”‚   â””â”€â”€ config.ts              # Configuration constants
â””â”€â”€ hooks/
    â””â”€â”€ use-prune-manager.ts   # Prune state management hook
```

## ğŸš€ Getting Started

### Prerequisites
- Node.js 18+
- pnpm (or npm/yarn)
- OpenAI API key

### Installation

```bash
cd app
npm install
```

### Environment Setup

Create `.env` file:

```env
OPENAI_API_KEY=your_openai_api_key_here
```

### Run Development Server

```bash
npm run dev
```

Open [http://localhost:3000](http://localhost:3000)

## ğŸ§ª Testing the Pruning Flow

1. **Build context**: "Get me the weather for New York, LA, Chicago, Miami, and Seattle"
2. **Synthesize**: "Which city has the best weather? Give me a comparison."
3. **Close topic**: "Perfect, I've noted that. The weather research is complete."
4. **Pivot**: "Now, what's the square root of 144?"

You should see:
- `âœ‚ï¸ PRUNE_SUGGESTION` events in the debug panel
- `ğŸ—‘ï¸ PRUNE_EXECUTED` events for approved suggestions
- `[Context Summary]` breadcrumb injected on subsequent requests

## ğŸ”§ Tech Stack

| Technology | Purpose |
|------------|---------|
| Next.js 16 | App framework |
| AI SDK v6 | LLM streaming + tool calling |
| OpenAI GPT-4o-mini | Language model |
| Tiktoken | Token counting |
| Tailwind CSS 4 | Styling |
| Radix UI | UI primitives |
| Zustand | State management |

## ğŸ“Š Key Features

- **Metadata Injection**: Messages tagged with `[msg:XXX][tokens:N][tally:N]`
- **Context Budget Display**: Visual progress bar showing token usage
- **Prune Suggestions**: Model outputs XML with message IDs and confidence scores
- **Context Summarization**: Full conversation summary preserved as breadcrumb
- **Debug Panel**: Real-time event logging for tool calls, messages, and pruning
- **Server-Side Filtering**: Pruned messages never sent to model on subsequent requests

## ğŸ“ How Pruning Works

### System Prompt Instructions

The model receives instructions to:
1. Monitor context budget
2. Suggest pruning when topics close or data is synthesized
3. Include a `<context_summary>` of the entire conversation
4. Format suggestions as XML with confidence scores

### Prune Suggestion Format

```xml
<prune_suggestions>
  <context_summary>User researched weather for 5 cities. Best: LA at 85Â°F. Then searched Austin restaurants.</context_summary>
  <suggestion id="msg:002" confidence="0.9" tokens="114" reason="Weather data synthesized" />
  <suggestion id="msg:004" confidence="0.85" tokens="286" reason="Comparison no longer needed" />
</prune_suggestions>
```

### Server-Side Execution

1. Parse suggestions from model response
2. Filter by confidence threshold (default: 0.8)
3. Store approved IDs + context summary in global state
4. On next request, skip pruned messages and inject summary

## ğŸ”® Future Improvements

- [ ] Persist prune state to database (currently in-memory)
- [ ] User approval UI for prune suggestions
- [ ] Configurable confidence thresholds per message type
- [ ] Migrate to `ToolLoopAgent` pattern for cleaner SDK integration
- [ ] A/B testing prune accuracy vs manual truncation

## ğŸ“š References

- [Teaching AI Agents to Forget to Stop Forgetting](https://medium.com/@phoenixrr2113/teaching-ai-agents-to-forget-to-stop-forgetting-what-speed-reading-taught-us-about-context-a2044c3f47d0) - Inspiration blog post
- [AI SDK Documentation](https://sdk.vercel.ai/docs) - Vercel AI SDK
- [Context Caching Strategies](https://platform.openai.com/docs/guides/context-caching) - OpenAI docs

## ğŸ“„ License

MIT
